{
    "sourceFile": "eett_scraper.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1748590819814,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1748590819814,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\n\"\"\"\nEETT Antenna Data Scraper\n\nA web scraper for extracting antenna installation data from the Greek National \nTelecommunications and Post Commission (EETT) website at https://keraies.eett.gr/\n\nThis scraper allows you to search for antenna installations by municipality\nand export the results to CSV or Excel format.\n\nAuthor: George Panagiotou\nLicense: MIT\n\"\"\"\n\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport time\nimport csv\nimport sys\nimport os\nfrom urllib.parse import urljoin\nimport re\nimport logging\nfrom typing import List, Dict, Optional\n\n\nclass EETTScraper:\n    \"\"\"\n    A scraper for the EETT (Greek Telecommunications Commission) antenna database.\n    \n    This class provides methods to search for antenna installations by municipality\n    and extract detailed information about each installation.\n    \"\"\"\n    \n    def __init__(self, debug: bool = False):\n        \"\"\"\n        Initialize the EETT scraper.\n        \n        Args:\n            debug (bool): Enable debug mode for verbose logging\n        \"\"\"\n        self.base_url = \"https://keraies.eett.gr/\"\n        self.search_url = \"https://keraies.eett.gr/anazhthsh.php\"\n        self.session = requests.Session()\n        self.debug = debug\n        \n        # Configure logging\n        log_level = logging.DEBUG if debug else logging.INFO\n        logging.basicConfig(\n            level=log_level,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n        self.logger = logging.getLogger(__name__)\n        \n        # Set headers to mimic a real browser\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'el-GR,el;q=0.9,en;q=0.8',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Referer': 'https://keraies.eett.gr/anazhthsh.php'\n        })\n    \n    def get_municipality_options(self) -> Dict[str, str]:\n        \"\"\"\n        Retrieve available municipality options from the search form.\n        \n        Returns:\n            Dict[str, str]: Dictionary mapping municipality names to their form values\n        \"\"\"\n        try:\n            response = self.session.get(self.search_url)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            municipality_select = soup.find('select', {'name': 'municipality'})\n            \n            if municipality_select:\n                options = {}\n                for option in municipality_select.find_all('option'):\n                    value = option.get('value', '')\n                    text = option.get_text(strip=True)\n                    if value and text and value != '':\n                        options[text] = value\n                return options\n            return {}\n            \n        except requests.RequestException as e:\n            self.logger.error(f\"Error getting municipality options: {e}\")\n            return {}\n\n    def search_municipality(self, municipality_name: str, max_pages: Optional[int] = None) -> List[Dict[str, str]]:\n        \"\"\"\n        Search for antenna data in a specific municipality.\n        \n        Args:\n            municipality_name (str): Name of the municipality to search\n            max_pages (Optional[int]): Maximum number of pages to scrape (None for all pages)\n        \n        Returns:\n            List[Dict[str, str]]: List of dictionaries containing antenna data\n        \"\"\"\n        self.logger.info(f\"Searching for antennas in municipality: {municipality_name}\")\n        \n        # Get municipality options to find the correct value\n        municipality_options = self.get_municipality_options()\n        municipality_value = self._find_municipality_value(municipality_name, municipality_options)\n        \n        if not municipality_value:\n            self.logger.error(f\"Municipality '{municipality_name}' not found\")\n            self._show_available_municipalities(municipality_options)\n            return []\n        \n        # Get search form structure\n        search_data = self._prepare_search_data(municipality_value)\n        if not search_data:\n            return []\n        \n        return self._scrape_all_pages(search_data, max_pages)\n    \n    def _find_municipality_value(self, municipality_name: str, municipality_options: Dict[str, str]) -> Optional[str]:\n        \"\"\"Find the form value for a given municipality name.\"\"\"\n        for name, value in municipality_options.items():\n            if municipality_name.lower() in name.lower() or name.lower() in municipality_name.lower():\n                self.logger.info(f\"Found municipality match: {name} (value: {value})\")\n                return value\n        return None\n    \n    def _show_available_municipalities(self, municipality_options: Dict[str, str]) -> None:\n        \"\"\"Display available municipality options.\"\"\"\n        self.logger.info(\"Available municipalities (showing first 10):\")\n        for name in list(municipality_options.keys())[:10]:\n            self.logger.info(f\"  - {name}\")\n    \n    def _prepare_search_data(self, municipality_value: str) -> Optional[Dict[str, str]]:\n        \"\"\"Prepare the search form data.\"\"\"\n        try:\n            response = self.session.get(self.search_url)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Get hidden form fields\n            hidden_inputs = soup.find_all('input', {'type': 'hidden'})\n            self.logger.debug(f\"Found {len(hidden_inputs)} hidden form fields\")\n            \n            search_data = {\n                'address': '',\n                'municipality': municipality_value,\n                'siteId': '',\n            }\n            \n            # Add hidden form fields\n            for hidden_input in hidden_inputs:\n                name = hidden_input.get('name')\n                value = hidden_input.get('value', '')\n                if name:\n                    search_data[name] = value\n            \n            self.logger.debug(f\"Search data prepared: {search_data}\")\n            return search_data\n            \n        except requests.RequestException as e:\n            self.logger.error(f\"Error accessing search page: {e}\")\n            return None\n    \n    def _scrape_all_pages(self, search_data: Dict[str, str], max_pages: Optional[int]) -> List[Dict[str, str]]:\n        \"\"\"Scrape all pages of results.\"\"\"\n        all_antennas = []\n        page_num = 1\n        \n        while True:\n            if max_pages and page_num > max_pages:\n                break\n                \n            self.logger.info(f\"Scraping page {page_num}...\")\n            \n            current_search_data = search_data.copy()\n            current_search_data['startPage'] = page_num\n            current_search_data['myAction'] = 'search' if page_num == 1 else 'page'\n            \n            try:\n                response = self._make_search_request(current_search_data)\n                if not response:\n                    break\n                \n                # Save debug files if in debug mode\n                if self.debug and page_num <= 2:\n                    self._save_debug_response(response, page_num)\n                \n                soup = BeautifulSoup(response.content, 'html.parser')\n                antennas_on_page = self._parse_results(soup)\n                \n                if not antennas_on_page:\n                    self.logger.warning(\"No antennas found on this page\")\n                    if self.debug:\n                        self._debug_page_structure(soup)\n                    if page_num == 1:\n                        self.logger.warning(\"First page returned no results\")\n                    break\n                \n                all_antennas.extend(antennas_on_page)\n                self.logger.info(f\"Found {len(antennas_on_page)} antennas on page {page_num}\")\n                \n                if not self._has_next_page(soup, page_num):\n                    break\n                    \n                page_num += 1\n                time.sleep(1)  # Be respectful to the server\n                \n            except requests.RequestException as e:\n                self.logger.error(f\"Error on page {page_num}: {e}\")\n                break\n        \n        self.logger.info(f\"Total antennas found: {len(all_antennas)}\")\n        return all_antennas\n    \n    def _make_search_request(self, search_data: Dict[str, str]) -> Optional[requests.Response]:\n        \"\"\"Make a search request to the server.\"\"\"\n        try:\n            response = self.session.post(\n                urljoin(self.base_url, \"getData.php\"),\n                data=search_data,\n                headers={\n                    'Content-Type': 'application/x-www-form-urlencoded',\n                    'Referer': self.search_url\n                }\n            )\n            response.raise_for_status()\n            return response\n        except requests.RequestException as e:\n            self.logger.error(f\"Search request failed: {e}\")\n            return None\n    \n    def _save_debug_response(self, response: requests.Response, page_num: int) -> None:\n        \"\"\"Save response HTML for debugging purposes.\"\"\"\n        filename = f'debug_response_page_{page_num}.html'\n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write(response.text)\n            self.logger.debug(f\"Saved response HTML to {filename}\")\n        except IOError as e:\n            self.logger.error(f\"Failed to save debug file: {e}\")\n    \n    def _parse_results(self, soup: BeautifulSoup) -> List[Dict[str, str]]:\n        \"\"\"Parse results from the HTML response.\"\"\"\n        tables = soup.find_all('table')\n        for table in tables:\n            antennas = self._parse_table_results(table)\n            if antennas:\n                self.logger.debug(f\"Successfully parsed {len(antennas)} antennas from table\")\n                return antennas\n        return []\n    \n    def _parse_table_results(self, table) -> List[Dict[str, str]]:\n        \"\"\"Parse antenna data from HTML table.\"\"\"\n        rows = table.find_all('tr')\n        if len(rows) < 2:\n            return []\n        \n        # Map table headers to field names\n        header_map = self._map_table_headers(rows[0])\n        if not self._validate_header_map(header_map):\n            return []\n        \n        self.logger.debug(f\"Found antenna table with headers: {header_map}\")\n        \n        # Parse data rows\n        antennas = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if len(cells) > max(header_map.values()):\n                antenna_data = self._extract_antenna_data(cells, header_map)\n                antennas.append(antenna_data)\n        \n        return antennas\n    \n    def _map_table_headers(self, header_row) -> Dict[str, int]:\n        \"\"\"Map table headers to column indices.\"\"\"\n        header_cells = header_row.find_all(['th', 'td'])\n        header_map = {}\n        \n        for i, cell in enumerate(header_cells):\n            text = cell.get_text(strip=True)\n            if 'Κωδ.' in text and 'θέσης' in text:\n                header_map['position_code'] = i\n            elif 'Κατηγορία' in text:\n                header_map['category'] = i\n            elif 'Εταιρία' in text:\n                header_map['company'] = i\n            elif 'Διεύθυνση' in text:\n                header_map['address'] = i\n            elif 'Δήμος' in text:\n                header_map['municipality'] = i\n            elif 'Κωδ. Θέσης' in text:\n                header_map['sequence'] = i\n        \n        return header_map\n    \n    def _validate_header_map(self, header_map: Dict[str, int]) -> bool:\n        \"\"\"Validate that required headers are present.\"\"\"\n        required_headers = ['position_code', 'company', 'address', 'municipality']\n        missing_headers = [h for h in required_headers if h not in header_map]\n        \n        if missing_headers:\n            self.logger.debug(f\"Missing required headers: {missing_headers}\")\n            return False\n        return True\n    \n    def _extract_antenna_data(self, cells, header_map: Dict[str, int]) -> Dict[str, str]:\n        \"\"\"Extract antenna data from table cells.\"\"\"\n        return {\n            'sequence': cells[header_map.get('sequence', 0)].get_text(strip=True) if 'sequence' in header_map else '',\n            'position_code': cells[header_map['position_code']].get_text(strip=True),\n            'category': cells[header_map.get('category', 0)].get_text(strip=True) if 'category' in header_map else '',\n            'company': cells[header_map['company']].get_text(strip=True),\n            'address': cells[header_map['address']].get_text(strip=True),\n            'municipality': cells[header_map['municipality']].get_text(strip=True)\n        }\n    \n    def _debug_page_structure(self, soup: BeautifulSoup) -> None:\n        \"\"\"Debug helper to understand page structure.\"\"\"\n        if not self.debug:\n            return\n            \n        self.logger.debug(\"Analyzing page structure...\")\n        \n        tables = soup.find_all('table')\n        self.logger.debug(f\"Found {len(tables)} tables\")\n        \n        # Check pagination\n        pagination_ul = soup.find('ul', class_='pagination')\n        if pagination_ul:\n            pagination_items = pagination_ul.find_all('li')\n            self.logger.debug(f\"Found pagination with {len(pagination_items)} items\")\n        \n        # Check for result indicators\n        page_text = soup.get_text().lower()\n        indicators = ['αποτελέσματα', 'σφάλμα', 'error', 'κεραία', 'antenna']\n        found_indicators = [ind for ind in indicators if ind in page_text]\n        if found_indicators:\n            self.logger.debug(f\"Found content indicators: {found_indicators}\")\n    \n    def _has_next_page(self, soup: BeautifulSoup, current_page_num: int) -> bool:\n        \"\"\"Check if there's a next page available.\"\"\"\n        pagination_ul = soup.find('ul', class_='pagination')\n        if not pagination_ul:\n            return False\n        \n        pagination_items = pagination_ul.find_all('li')\n        \n        # Look for next page indicators\n        for li in pagination_items:\n            # Check for \"Next Page\" link\n            title = li.get('title', '')\n            classes = li.get('class', [])\n            \n            if ('Επόμενη' in title or 'Next' in title) and 'disabled' not in classes:\n                return True\n            \n            # Check for page numbers greater than current\n            a_tag = li.find('a')\n            if a_tag:\n                text = a_tag.get_text(strip=True)\n                onclick = a_tag.get('onclick', '')\n                \n                if text.isdigit() and int(text) > current_page_num:\n                    return True\n                \n                # Check onclick for page numbers\n                match = re.search(r\"startPage\\.value='?(\\d+)'?\", onclick)\n                if match and int(match.group(1)) > current_page_num:\n                    return True\n        \n        return False\n    \n    def save_to_csv(self, data: List[Dict[str, str]], filename: str = 'antenna_data.csv') -> None:\n        \"\"\"\n        Save the scraped data to a CSV file.\n        \n        Args:\n            data: List of antenna data dictionaries\n            filename: Output filename\n        \"\"\"\n        if not data:\n            self.logger.warning(\"No data to save\")\n            return\n        \n        try:\n            fieldnames = ['sequence', 'position_code', 'category', 'company', 'address', 'municipality']\n            \n            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n                writer.writerows(data)\n            \n            self.logger.info(f\"Data saved to {filename}\")\n            \n        except IOError as e:\n            self.logger.error(f\"Error saving CSV file: {e}\")\n    \n    def save_to_excel(self, data: List[Dict[str, str]], filename: str = 'antenna_data.xlsx') -> None:\n        \"\"\"\n        Save the scraped data to an Excel file.\n        \n        Args:\n            data: List of antenna data dictionaries\n            filename: Output filename\n        \"\"\"\n        if not data:\n            self.logger.warning(\"No data to save\")\n            return\n        \n        try:\n            df = pd.DataFrame(data)\n            df.to_excel(filename, index=False)\n            self.logger.info(f\"Data saved to {filename}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Error saving Excel file: {e}\")\n\n\ndef main():\n    \"\"\"Main function to run the scraper.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Scrape antenna data from EETT website',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s \"Χαλκιδέων\"                    # Scrape all pages for Chalkida\n  %(prog)s \"Αθηναίων\" --max-pages 5       # Scrape first 5 pages for Athens\n  %(prog)s --list                         # Show available municipalities\n  %(prog)s \"Θεσσαλονίκης\" --debug         # Enable debug mode\n        \"\"\"\n    )\n    \n    parser.add_argument('municipality', nargs='?', \n                       help='Municipality name to search')\n    parser.add_argument('--list', '-l', action='store_true',\n                       help='List available municipalities')\n    parser.add_argument('--max-pages', type=int, \n                       help='Maximum number of pages to scrape')\n    parser.add_argument('--debug', action='store_true',\n                       help='Enable debug mode')\n    parser.add_argument('--output-dir', default='.',\n                       help='Output directory for files (default: current directory)')\n    \n    args = parser.parse_args()\n    \n    if args.list:\n        scraper = EETTScraper(debug=args.debug)\n        print(\"Available municipalities:\")\n        options = scraper.get_municipality_options()\n        for name in sorted(options.keys()):\n            print(f\"  - {name}\")\n        return\n    \n    if not args.municipality:\n        parser.error(\"Municipality name is required (use --list to see available options)\")\n    \n    # Create output directory if it doesn't exist\n    if args.output_dir != '.' and not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    scraper = EETTScraper(debug=args.debug)\n    \n    try:\n        # Search for antennas\n        antenna_data = scraper.search_municipality(args.municipality, args.max_pages)\n        \n        if antenna_data:\n            # Generate safe filename\n            safe_filename = re.sub(r'[^\\w\\s-]', '', args.municipality).strip()\n            safe_filename = re.sub(r'[-\\s]+', '_', safe_filename)\n            \n            # Create file paths\n            csv_filename = os.path.join(args.output_dir, f\"antennas_{safe_filename}.csv\")\n            excel_filename = os.path.join(args.output_dir, f\"antennas_{safe_filename}.xlsx\")\n            \n            # Save files\n            scraper.save_to_csv(antenna_data, csv_filename)\n            scraper.save_to_excel(antenna_data, excel_filename)\n            \n            # Print summary\n            print(f\"\\n{'='*50}\")\n            print(f\"SCRAPING SUMMARY\")\n            print(f\"{'='*50}\")\n            print(f\"Municipality: {args.municipality}\")\n            print(f\"Total antennas: {len(antenna_data)}\")\n            print(f\"Files saved:\")\n            print(f\"  - {csv_filename}\")\n            print(f\"  - {excel_filename}\")\n            \n            # Show sample data\n            if antenna_data:\n                print(f\"\\nSample data (first antenna):\")\n                print(\"-\" * 30)\n                for key, value in antenna_data[0].items():\n                    print(f\"  {key}: {value}\")\n                print(\"-\" * 30)\n        else:\n            print(f\"\\nNo antenna data found for municipality: {args.municipality}\")\n            print(\"\\nTroubleshooting suggestions:\")\n            print(\"1. Check the municipality name spelling\")\n            print(\"2. Use --list to see available municipalities\")\n            print(\"3. Try using --debug for more information\")\n    \n    except KeyboardInterrupt:\n        print(\"\\nScraping interrupted by user.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        if args.debug:\n            import traceback\n            traceback.print_exc()\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
    ]
}